---
title: "Normalization"
author: "Karol Nowicki-Osuch, based on Nils Eling code"
date: "`r Sys.Date()`"
output: 
html_document:
    toc: true
    toc_float:
      toc_collapsed: true
    toc_depth: 3
    number_sections: false
    theme: paper
    code_folding: hide
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, 
        encoding = encoding, output_file = '~/Dropbox/Postdoc/2019-12-29_BE2020/Results/Reports/2.4_Normalization_filtered.html') })
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# export RSTUDIO_PANDOC=/usr/lib/rstudio-server/bin/pandoc
# Rscript -e "library(rmarkdown); render('2.4_Normalization_filtered.Rmd')"
knitr::opts_chunk$set(echo = TRUE)
```

# Read in the data

```{r libraries, message=FALSE}
library(scran)
library(scater)
```

We use the filtered data to perform normalization.
Here, raw counts are normalized within each batch, which is important for batch correction. 
To visualize counts across batches, we normalize raw counts across the selected cells.

```{r data, eval=TRUE}
sce.list <- readRDS("~/Dropbox/Postdoc/2019-12-29_BE2020/All_sce_filtered.rds")

for(i in 1:length(sce.list)){
  cur_sce <- sce.list[[i]]
  
  if("logcounts" %in% names(cur_sce@assays)){next}
  
  clusters <- quickCluster(cur_sce, method = "igraph",# irlba.args = c("work" = 100), 
                        min.size = 50, use.ranks=TRUE)
 
                         #max.size = 2000, min.size = 50)

  cur_sce <- computeSumFactors(cur_sce, clusters=clusters)

  cur_sce <- logNormCounts(cur_sce, log = TRUE)
  
  sce.list[[i]] <- cur_sce
}

saveRDS(sce.list, "~/Dropbox/Postdoc/2019-12-29_BE2020/All_sce_filtered.rds")
```

# End Matter

To finish get session info:

```{r Endnote, include=TRUE, echo=TRUE, warning=FALSE, eval=TRUE, message=FALSE, tidy=TRUE}
sessionInfo()
```

